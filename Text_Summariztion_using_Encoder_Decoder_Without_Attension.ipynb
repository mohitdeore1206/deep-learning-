{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Summarization\n",
        "\n",
        "\n",
        "\n",
        "Encoder and decoder without attention\n",
        "\n",
        "\n",
        "1. Encoder\n",
        "\n",
        "The encoder processes the input sequence and compresses it into a fixed-size context vector (the final hidden state).\n",
        "\n",
        "Steps:\n",
        "\n",
        "a) Embed Input Tokens: Convert input tokens into dense vectors using an embedding layer.\n",
        "\n",
        "b) Process Through RNN/LSTM/GRU: Pass the embeddings through a recurrent layer.\n",
        "\n",
        "c) Output Context Vector: Use the final hidden state(s) as the context for the decoder\n",
        "\n",
        "\n",
        "2. Decoder\n",
        "\n",
        "The decoder generates the output sequence token by token, starting with the initial input token (e.g., <start>).\n",
        "\n",
        "Steps:\n",
        "\n",
        "a) Embed Input Token: Convert the current input token into dense vectors.\n",
        "\n",
        "b) Combine With Context Vector: Initialize the decoder's hidden and cell states with the encoder's final hidden and cell states.\n",
        "\n",
        "c) Generate Output Token: Predict the next token using the recurrent layer and a linear layer."
      ],
      "metadata": {
        "id": "EMGRf49fB6_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "!pip install rouge-score # Install the 'rouge-score' package\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load Dataset\n",
        "file_path = '/content/Text_summarization.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Data columns: 'id', 'article', 'highlights'\n",
        "id = data['id'].values\n",
        "articles = data['article'].values\n",
        "highlights = data['highlights'].values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(articles, highlights, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizer for input and output\n",
        "max_vocab_size = 20000\n",
        "max_input_len = 300  # Max length for articles\n",
        "max_output_len = 50  # Max length for highlights\n",
        "\n",
        "input_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<UNK>')\n",
        "input_tokenizer.fit_on_texts(X_train)\n",
        "output_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<UNK>')\n",
        "output_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "# Convert text to sequences and pad them\n",
        "X_train_seq = input_tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = input_tokenizer.texts_to_sequences(X_test)\n",
        "y_train_seq = output_tokenizer.texts_to_sequences(y_train)\n",
        "y_test_seq = output_tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_input_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_input_len, padding='post')\n",
        "y_train_padded = pad_sequences(y_train_seq, maxlen=max_output_len, padding='post')\n",
        "y_test_padded = pad_sequences(y_test_seq, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# Vocabulary sizes\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TEcM7GmIq4h",
        "outputId": "69789e19-eebc-4f29-90f5-adee5f1315fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dim = 128\n",
        "\n",
        "# Build Encoder-Decoder Model (Without Attention)\n",
        "def build_encoder_decoder():\n",
        "    encoder_input = Input(shape=(max_input_len,))\n",
        "    # The following line is changed to use max_output_len - 1\n",
        "    decoder_input = Input(shape=(max_output_len - 1,)) #  Decoder input shape should match decoder_input_data\n",
        "\n",
        "    # Embedding\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim, mask_zero=True)(encoder_input)\n",
        "    decoder_embedding = Embedding(output_vocab_size, embedding_dim, mask_zero=True)(decoder_input)\n",
        "\n",
        "    # Encoder\n",
        "    encoder_lstm = LSTM(256, return_state=True)\n",
        "    _, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "    decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Output layer\n",
        "    dense = Dense(output_vocab_size, activation='softmax')\n",
        "    output = dense(decoder_output)\n",
        "\n",
        "    # Reshape the output to match the shape of y_train_padded for sparse_categorical_crossentropy\n",
        "    # output = tf.reshape(output, (-1, output_vocab_size)) # Original shape: (batch_size, max_output_len, output_vocab_size)\n",
        "    # output = Reshape((-1, output_vocab_size))(output) # This line is causing the shape mismatch\n",
        "\n",
        "    return Model([encoder_input, decoder_input], output)\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_encoder_decoder()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Prepare Decoder Input and Output\n",
        "decoder_input_data = y_train_padded[:, :-1] # Decoder input is shifted target sequence\n",
        "decoder_target_data = y_train_padded[:, 1:] # Decoder target is the original target sequence shifted by one\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Cg0JaiWnIsiT",
        "outputId": "1bf41499-f6b0-47b2-a74d-abed6b60ff72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m50,644,224\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m15,230,720\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m394,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m394,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m118990\u001b[0m)     │     \u001b[38;5;34m30,580,430\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">50,644,224</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,230,720</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118990</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,580,430</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m97,243,854\u001b[0m (370.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,243,854</span> (370.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m97,243,854\u001b[0m (370.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,243,854</span> (370.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-13-79fed74d1c3d\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming X_train_seq contains your input sequences\n",
        "# Use the same max_input_len as defined in the model\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_input_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "LOS1xjQPKT3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-14-79fed74d1c3d\n",
        "# Prepare Decoder Input and Output\n",
        "# Assuming y_train_padded contains your target sequences\n",
        "decoder_input_data = y_train_padded[:, :-1]\n",
        "decoder_target_data = y_train_padded[:, 1:]"
      ],
      "metadata": {
        "id": "XmSuvbGCKZDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-25-375bd95fc266\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# **Replace 'summary' with the actual column name in your 'data' DataFrame**\n",
        "# For example, if the column is named 'text', use:\n",
        "y_train_texts = data['text'].iloc[y_train_padded.index].tolist()\n",
        "\n",
        "# 1. Create a Tokenizer instance\n",
        "tokenizer = Tokenizer(num_words=output_vocab_size, oov_token='<OOV>') # Set num_words to your desired vocabulary size\n",
        "\n",
        "# 2. Fit the tokenizer on your training data\n",
        "tokenizer.fit_on_texts(y_train_texts)\n",
        "\n",
        "# Re-tokenize target sequences using the same tokenizer as the decoder\n",
        "y_train_seq = tokenizer.texts_to_sequences(y_train_texts)\n",
        "y_train_padded = pad_sequences(y_train_seq, maxlen=max_output_len, padding='post', truncating='post')\n",
        "\n",
        "# Prepare Decoder Input and Output\n",
        "decoder_input_data = y_train_padded[:, :-1]\n",
        "decoder_target_data = y_train_padded[:, 1:]"
      ],
      "metadata": {
        "id": "Hwbixlj_Ld3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clip out-of-range indices to the maximum valid index\n",
        "decoder_input_data = np.clip(decoder_input_data, 0, output_vocab_size - 1)"
      ],
      "metadata": {
        "id": "W3I78ew-LCAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-30-e4a6b6322cd3\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Replace 'text' with the actual column name containing your target text\n",
        "# Check the column names in your 'data' DataFrame using data.columns\n",
        "target_column_name = 'highlights'  # Replace 'your_actual_column_name'\n",
        "\n",
        "# Assuming y_train_padded is a NumPy array and you want to select rows from data\n",
        "# based on the rows represented by y_train_padded, you can use a range of indices:\n",
        "\n",
        "# Get the number of rows in y_train_padded\n",
        "num_samples = y_train_padded.shape[0]\n",
        "\n",
        "# Create a range of indices corresponding to the rows in y_train_padded\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "# Use these indices to select the corresponding rows from data\n",
        "y_train_texts = data[target_column_name].iloc[indices].tolist()\n",
        "\n",
        "# 1. Create a Tokenizer instance\n",
        "tokenizer = Tokenizer(num_words=output_vocab_size, oov_token='<OOV>') # Set num_words to your desired vocabulary size\n",
        "\n",
        "# 2. Fit the tokenizer on your training data\n",
        "tokenizer.fit_on_texts(y_train_texts)\n",
        "\n",
        "# Re-tokenize target sequences using the same tokenizer as the decoder\n",
        "y_train_seq = tokenizer.texts_to_sequences(y_train_texts)\n",
        "y_train_padded = pad_sequences(y_train_seq, maxlen=max_output_len, padding='post', truncating='post')\n",
        "\n",
        "# Prepare Decoder Input and Output\n",
        "decoder_input_data = y_train_padded[:, :-1]\n",
        "decoder_target_data = y_train_padded[:, 1:]\n",
        "\n",
        "# Clip out-of-range indices to the maximum valid index before training\n",
        "decoder_input_data = np.clip(decoder_input_data, 0, output_vocab_size - 1)\n",
        "decoder_target_data = np.clip(decoder_target_data, 0, output_vocab_size - 1) # clip target as well/////////////"
      ],
      "metadata": {
        "id": "-Z34WXb_Katl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode sequence\n",
        "def decode_sequence(sequence, tokenizer):\n",
        "    \"\"\"Convert token IDs to text.\"\"\"\n",
        "    reverse_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
        "    return \" \".join([reverse_vocab.get(token, '') for token in sequence if token != 0])////////////////\n"
      ],
      "metadata": {
        "id": "0pT-jUUPQkbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}